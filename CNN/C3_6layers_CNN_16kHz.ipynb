{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C3_6layers_CNN_16kHz.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d3734e95f63c4f0ea5ef888e8c2e52c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_64e209e66f944b5486f429eb71ce288d","IPY_MODEL_e2340f61a0fa4bc983770ea2d6610e1e","IPY_MODEL_6da07019ecb549de81c1429b57c704bf"],"layout":"IPY_MODEL_fd8d28f444bd4f57a30cd9462ae019d5"}},"64e209e66f944b5486f429eb71ce288d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3096d5911d94c9ab91239f4bda24a10","placeholder":"​","style":"IPY_MODEL_36720bf5df7841d49ef3c05557c24d67","value":"100%"}},"e2340f61a0fa4bc983770ea2d6610e1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dbad85d6da544a6810a1f4e17fe45b9","max":2428923189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f83e21c37137418eb19609a4d1b276a6","value":2428923189}},"6da07019ecb549de81c1429b57c704bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5b9048cefbc43ed92e78b28af879648","placeholder":"​","style":"IPY_MODEL_f434086b253b45e999c1327195253141","value":" 2.26G/2.26G [00:17&lt;00:00, 219MB/s]"}},"fd8d28f444bd4f57a30cd9462ae019d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3096d5911d94c9ab91239f4bda24a10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36720bf5df7841d49ef3c05557c24d67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dbad85d6da544a6810a1f4e17fe45b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f83e21c37137418eb19609a4d1b276a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5b9048cefbc43ed92e78b28af879648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f434086b253b45e999c1327195253141":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-hmEorIATH3h","executionInfo":{"status":"ok","timestamp":1650268828268,"user_tz":-120,"elapsed":8901,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import sys\n","\n","import math\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","\n","from tqdm import tqdm\n","\n","from torchaudio.datasets import SPEECHCOMMANDS\n","import os"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvWMAE3uTZcr","executionInfo":{"status":"ok","timestamp":1650268828270,"user_tz":-120,"elapsed":11,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}},"outputId":"61608b62-8bc6-4c26-c630-b59d5c05c6bb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["class SubsetSC(SPEECHCOMMANDS):\n","    def __init__(self, subset: str = None):\n","        super().__init__(\"./\", download=True)\n","\n","        def load_list(filename):\n","            filepath = os.path.join(self._path, filename)\n","            with open(filepath) as fileobj:\n","                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n","\n","        if subset == \"validation\":\n","            self._walker = load_list(\"validation_list.txt\")\n","        elif subset == \"testing\":\n","            self._walker = load_list(\"testing_list.txt\")\n","        elif subset == \"training\":\n","            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n","            excludes = set(excludes)\n","            self._walker = [w for w in self._walker if w not in excludes]\n","\n","\n","# Create training and testing split of the data. We do not use validation in this tutorial.\n","train_set = SubsetSC(\"training\")\n","val_set = SubsetSC(\"validation\")\n","test_set = SubsetSC(\"testing\")\n","\n","waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"],"metadata":{"id":"lZ-j7HArTaci","executionInfo":{"status":"ok","timestamp":1650268930044,"user_tz":-120,"elapsed":101780,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["d3734e95f63c4f0ea5ef888e8c2e52c3","64e209e66f944b5486f429eb71ce288d","e2340f61a0fa4bc983770ea2d6610e1e","6da07019ecb549de81c1429b57c704bf","fd8d28f444bd4f57a30cd9462ae019d5","c3096d5911d94c9ab91239f4bda24a10","36720bf5df7841d49ef3c05557c24d67","2dbad85d6da544a6810a1f4e17fe45b9","f83e21c37137418eb19609a4d1b276a6","c5b9048cefbc43ed92e78b28af879648","f434086b253b45e999c1327195253141"]},"outputId":"01d2ef9d-8e6f-4020-9fa1-437aa8917108"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/2.26G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3734e95f63c4f0ea5ef888e8c2e52c3"}},"metadata":{}}]},{"cell_type":"code","source":["labels = sorted(list(set(datapoint[2] for datapoint in train_set)))"],"metadata":{"id":"OAqY3k-tTicX","executionInfo":{"status":"ok","timestamp":1650268951596,"user_tz":-120,"elapsed":21563,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def label_to_index(word):\n","    # Return the position of the word in labels\n","    return torch.tensor(labels.index(word))\n","\n","\n","def index_to_label(index):\n","    # Return the word corresponding to the index in labels\n","    # This is the inverse of label_to_index\n","    return labels[index]"],"metadata":{"id":"9KUhC97KTs04","executionInfo":{"status":"ok","timestamp":1650268951597,"user_tz":-120,"elapsed":6,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n","    return batch.permute(0, 2, 1)\n","\n","\n","def collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # waveform, sample_rate, label, speaker_id, utterance_number\n","\n","    tensors, targets = [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for waveform, _, label, *_ in batch:\n","        tensors += [waveform]\n","        targets += [label_to_index(label)]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = pad_sequence(tensors)\n","    targets = torch.stack(targets)\n","\n","    return tensors, targets\n","\n","\n","batch_size = 256\n","\n","if device == \"cuda\":\n","    num_workers = 1\n","    pin_memory = True\n","else:\n","    num_workers = 0\n","    pin_memory = False\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")\n","val_loader = torch.utils.data.DataLoader(\n","    val_set,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")\n","test_loader = torch.utils.data.DataLoader(\n","    test_set,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")"],"metadata":{"id":"icXGz3JbTwkf","executionInfo":{"status":"ok","timestamp":1650268951597,"user_tz":-120,"elapsed":4,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class M5(nn.Module):\n","    def __init__(self, n_input=1, n_output=35, stride=16, n_channel=128):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n","        self.bn1 = nn.BatchNorm1d(n_channel)\n","        self.pool1 = nn.MaxPool1d(4)\n","        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n","        self.bn2 = nn.BatchNorm1d(n_channel)\n","        self.pool2 = nn.MaxPool1d(4)\n","        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n","        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n","        self.pool3 = nn.MaxPool1d(4)\n","        self.conv4 = nn.Conv1d(2 * n_channel, 4 * n_channel, kernel_size=3)\n","        self.bn4 = nn.BatchNorm1d(4 * n_channel)\n","        self.pool4 = nn.MaxPool1d(4)\n","        self.conv5 = nn.Conv1d(4 * n_channel, 8 * n_channel, kernel_size=1)\n","        self.bn5 = nn.BatchNorm1d(8 * n_channel)\n","        self.conv6 = nn.Conv1d(8 * n_channel, 16 * n_channel, kernel_size=1)\n","        self.bn6 = nn.BatchNorm1d(16 * n_channel)\n","        self.fc1 = nn.Linear(16 * n_channel, n_output)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(self.bn1(x))\n","        x = self.pool1(x)\n","        x = self.conv2(x)\n","        x = F.relu(self.bn2(x))\n","        x = self.pool2(x)\n","        x = self.conv3(x)\n","        x = F.relu(self.bn3(x))\n","        x = self.pool3(x)\n","        x = self.conv4(x)\n","        x = F.relu(self.bn4(x))\n","        x = self.pool4(x)\n","        x = self.conv5(x)\n","        x = F.relu(self.bn5(x))\n","        x = self.conv6(x)\n","        x = F.relu(self.bn6(x))\n","\n","        x = F.avg_pool1d(x, x.shape[-1])\n","        x = x.permute(0, 2, 1)\n","        x = self.fc1(x)\n","        return F.log_softmax(x, dim=2)\n","\n","\n","model = M5(n_input=waveform.shape[0], n_output=len(labels))\n","model.to(device)\n","#print(model)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","n = count_parameters(model)\n","print(\"Number of parameters: %s\" % n)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TeCeOt3KTze_","executionInfo":{"status":"ok","timestamp":1650268951920,"user_tz":-120,"elapsed":327,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}},"outputId":"1c187d9b-3db4-4252-a33a-43141bfb2c2f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of parameters: 3256355\n"]}]},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"],"metadata":{"id":"E-WlFqbpT470","executionInfo":{"status":"ok","timestamp":1650268951920,"user_tz":-120,"elapsed":4,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def train(epoch, log_interval):\n","    model.train()\n","\n","    total_loss = 0.\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        output = model(data)\n","\n","        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n","        loss = F.nll_loss(output.squeeze(), target)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print training stats\n","        if batch_idx % log_interval == 0:\n","            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n","\n","        # record loss\n","        losses.append(loss.item())\n","        "],"metadata":{"id":"Dn4Vrf84T753","executionInfo":{"status":"ok","timestamp":1650268951921,"user_tz":-120,"elapsed":4,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def number_of_correct(pred, target):\n","    # count number of correct predictions\n","    return pred.squeeze().eq(target).sum().item()\n","\n","\n","def get_likely_index(tensor):\n","    # find most likely label index for each element in the batch\n","    return tensor.argmax(dim=-1)\n","\n","@torch.no_grad()\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0.\n","    correct = 0\n","    n = 0\n","    for data, target in data_source:\n","\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        output = model(data)\n","\n","        total_loss += target.numel() * F.nll_loss(output.squeeze(), target).item()\n","        n += target.numel()\n","\n","\n","        pred = get_likely_index(output)\n","        correct += number_of_correct(pred, target)\n","\n","        #print(pred)\n","        #print(target)\n","    \n","\n","    accuracy = 100 * correct / len(test_loader.dataset)\n","    \n","    return total_loss / n, accuracy\n","\n","        "],"metadata":{"id":"zdxbfD2JT-ff","executionInfo":{"status":"ok","timestamp":1650268951921,"user_tz":-120,"elapsed":4,"user":{"displayName":"Unai Vicente","userId":"16589365669417155780"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["best_val_loss = float(\"inf\")\n","losses = []\n","\n","log_interval = 100\n","n_epoch = 10\n","\n","for epoch in range(1, n_epoch + 1):\n","    train(epoch, log_interval)\n","    val_loss, val_acc = evaluate(val_loader)\n","\n","    print(f'| Validation | val loss {val_loss:5.2f} | val accuracy {val_acc:.0f}%')\n","\n","    if val_loss < best_val_loss:\n","      with open(\"best_checkpoint_CNN.pth\", 'wb') as f:\n","        torch.save(model, f)\n","      best_val_loss = val_loss\n","    scheduler.step()\n","\n","with open(\"best_checkpoint_CNN.pth\", 'rb') as f:\n","  model = torch.load(f)\n","\n","with torch.no_grad():\n","  test_loss, test_acc = evaluate(test_loader)\n","    \n","#print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | test accuracy {test_acc:.0f}%')\n","#print('=' * 89)\n","\n","# Let's plot the training loss versus the number of iteration.\n","#plt.plot();\n","#plt.title(\"training loss\");"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZCQBPGpUBOI","outputId":"17e6a758-75fd-4f05-ad17-9130243b4ca9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/84843 (0%)]\tLoss: 3.615984\n","Train Epoch: 1 [25600/84843 (30%)]\tLoss: 2.909301\n","Train Epoch: 1 [51200/84843 (60%)]\tLoss: 1.506730\n"]}]},{"cell_type":"code","source":["print(f'| End of training | test loss {test_loss:5.2f} | test accuracy {test_acc:.0f}%')\n","\n","plt.plot(losses);\n","plt.title(\"training loss\");"],"metadata":{"id":"GSMX387yW_U5"},"execution_count":null,"outputs":[]}]}
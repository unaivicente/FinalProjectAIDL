{"cells":[{"cell_type":"markdown","source":["# GRU-2\n","\n","## Hyperparameter tuning\n","\n","- 1D convolutional layer + batch normalization + maxPooling : 4\n","\n","- Hidden dimension : 128\n","\n","- Number of channels : 128\n","\n","- Loss function : Negative Log Likelihood\n","\n","- Training epochs : 40\n","\n","- Batch size : 50\n","\n","- Learning rate : 0.01\n","\n","- Optimizer : SGD with 0.0001 weight decay and StepLR that decays the LR of each parameter group by 0.1 each 20 epochs.\n"],"metadata":{"id":"r-gGurmacWIq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1fXVpAVfevt"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchaudio\n","import sys\n","\n","import numpy as np\n","\n","import math\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","\n","from tqdm import tqdm\n","\n","from torchaudio.datasets import SPEECHCOMMANDS\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkikOxvXiM5M","executionInfo":{"status":"ok","timestamp":1650724628966,"user_tz":-120,"elapsed":368,"user":{"displayName":"Miriam Martínez","userId":"00485205967731265435"}},"outputId":"0cd5ca19-8fb6-4059-a733-5677ebad66a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["#Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","source":["# Split of the dataset and data preparation\n","The class SubsetSC splits the Speech Commands Dataset into train, validation and test sets. The classification of the audio files in each set is specified in the 'validation.txt' and 'testing.txt' files where are written the paths of them."],"metadata":{"id":"4cXu0dzV1QMZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oa227SafiPoW"},"outputs":[],"source":["class SubsetSC(SPEECHCOMMANDS):\n","    def __init__(self, subset: str = None):\n","        super().__init__(\"./\", download=True)\n","\n","        def load_list(filename):\n","            filepath = os.path.join(self._path, filename)\n","            with open(filepath) as fileobj:\n","                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n","\n","        if subset == \"validation\":\n","            self._walker = load_list(\"validation_list.txt\")\n","        elif subset == \"testing\":\n","            self._walker = load_list(\"testing_list.txt\")\n","        elif subset == \"training\":\n","            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n","            excludes = set(excludes)\n","            self._walker = [w for w in self._walker if w not in excludes]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["acb38c2ccc224967bf0e44159415414a","a422656a40854eb5b3225dd7cf2d1020","6efaa17e51604c90822fcee20182982e","1417227d21d44b31b461306b8c3450a7","ba0352d0e7a84838ba101edd29469b86","08f334c186924258bca5018cc21522bd","db28b2fd3bf44fd297bcff8e3f9a3f40","fd00c3038c984f6082b862976c60ad37","bc288b5de9c44f9f8d8cb8a66c3bf5cc","108998e2831b44588a2e751f14f83382","1db21f9d62c64b14a6cc0738d7583a59"]},"id":"bTGbXktIiRy5","executionInfo":{"status":"ok","timestamp":1650724731220,"user_tz":-120,"elapsed":102274,"user":{"displayName":"Miriam Martínez","userId":"00485205967731265435"}},"outputId":"28d13da7-e3a8-4bf7-d0a4-3134b188cd12"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/2.26G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb38c2ccc224967bf0e44159415414a"}},"metadata":{}}],"source":["# Create training, validation and testing split of the data\n","train_set = SubsetSC(\"training\")\n","val_set = SubsetSC(\"validation\")\n","test_set = SubsetSC(\"testing\")\n","\n","#Tuple made of waveform, sample rate, label (spoken word), speaker ID and number of the utterance.\n","waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXSkhxzuiTO4"},"outputs":[],"source":["#List of labels in the dataset (commands said by speakers)\n","labels = sorted(list(set(datapoint[2] for datapoint in train_set)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TtKgOvZiVH1"},"outputs":[],"source":["#Downsampling from 16kHz to 8kHz\n","new_sample_rate = 8000\n","transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n","transformed = transform(waveform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sb6IEUAkiWi7"},"outputs":[],"source":["def label_to_index(word):\n","    # Return the position of the word in labels\n","    return torch.tensor(labels.index(word))\n","\n","\n","def index_to_label(index):\n","    # Return the word corresponding to the index in labels\n","    # This is the inverse of label_to_index\n","    return labels[index]"]},{"cell_type":"markdown","source":["# Batched tensors for the model\n","To transform an array of data point made of audio and labels into two batched tensors for the model. The function collate_fn is used by the PyTorch DataLoader that allows us to iterate over a dataset by batches."],"metadata":{"id":"CcW-4eHx1uih"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r32fJfVkiYSe"},"outputs":[],"source":["def pad_sequence(batch):\n","    # Make all tensor in a batch the same length by padding with zeros\n","    batch = [item.t() for item in batch]\n","    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n","    return batch.permute(0, 2, 1)\n","\n","\n","def collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # waveform, sample_rate, label, speaker_id, utterance_number\n","\n","    tensors, targets = [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for waveform, _, label, *_ in batch:\n","        tensors += [waveform]\n","        targets += [label_to_index(label)]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = pad_sequence(tensors)\n","    targets = torch.stack(targets)\n","\n","    return tensors, targets\n","\n","\n","batch_size = 50\n","\n","if device == \"cuda\":\n","    num_workers = 1\n","    pin_memory = True\n","else:\n","    num_workers = 0\n","    pin_memory = False\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_set,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_set,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=collate_fn,\n","    num_workers=num_workers,\n","    pin_memory=pin_memory,\n",")"]},{"cell_type":"markdown","source":["# GRU-2 Model"],"metadata":{"id":"BfDKY7oy1ycb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKuz14-SibWt","executionInfo":{"status":"ok","timestamp":1650724762850,"user_tz":-120,"elapsed":8938,"user":{"displayName":"Miriam Martínez","userId":"00485205967731265435"}},"outputId":"99de1539-b839-4b7f-c82b-b617ce0106e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of parameters: 509347\n"]}],"source":["class GRU_2(nn.Module):\n","    def __init__(self, n_input=1, n_output=35, stride=3, hidden_dim=128, n_channel=128, nhead=4, num_layer=4):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n","        self.bn1 = nn.BatchNorm1d(n_channel)\n","        self.pool1 = nn.MaxPool1d(4)\n","        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n","        self.bn2 = nn.BatchNorm1d(n_channel)\n","        self.pool2 = nn.MaxPool1d(4)\n","        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n","        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n","        self.pool3 = nn.MaxPool1d(4)\n","        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n","        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n","        self.pool4 = nn.MaxPool1d(4)\n","        self.GRU = nn.GRU(2 * n_channel, hidden_dim, batch_first=True)\n","        self.fc1 = nn.Linear(hidden_dim, n_output)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(self.bn1(x))\n","        x = self.pool1(x)\n","        x = self.conv2(x)\n","        x = F.relu(self.bn2(x))\n","        x = self.pool2(x)\n","        x = self.conv3(x)\n","        x = F.relu(self.bn3(x))\n","        x = self.pool3(x)\n","        x = self.conv4(x)\n","        x = F.relu(self.bn4(x))\n","        x = self.pool4(x)\n","        x = x.permute(0, 2, 1)\n","        x, hnn = self.GRU(x)\n","        x = self.fc1(hnn[-1])\n","        return F.log_softmax(x, dim=-1)\n","\n","model = GRU_2(n_input=transformed.shape[0], n_output=len(labels))\n","model.cuda()\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","n = count_parameters(model)\n","print(\"Number of parameters: %s\" % n)"]},{"cell_type":"markdown","source":["# Training and testing\n","In this case we use SGD optimizer without weight decay and Negative Log Likelihood as loss function."],"metadata":{"id":"sVAhcy5-14Q-"}},{"cell_type":"code","source":["lr = 0.01"],"metadata":{"id":"81AcoP1K1wqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHSMT4RwigeC"},"outputs":[],"source":["optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=0.0001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eywGFApii0y"},"outputs":[],"source":["def train(epoch, log_interval):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","\n","        data = data.cuda()\n","        target = target.cuda()\n","\n","        # apply transform and model on whole batch directly on device\n","        data = transform(data)\n","        output = model(data)\n","\n","        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n","        loss = F.nll_loss(output.squeeze(), target)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print training stats\n","        if batch_idx % log_interval == 0:\n","            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n","\n","        # record loss\n","        losses.append(loss.item())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RCUpCPCijwv"},"outputs":[],"source":["def number_of_correct(pred, target):\n","    # count number of correct predictions\n","    return pred.squeeze().eq(target).sum().item()\n","\n","\n","def get_likely_index(tensor):\n","    # find most likely label index for each element in the batch\n","    return tensor.argmax(dim=-1)\n","\n","\n","@torch.no_grad()\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0.\n","    correct = 0\n","    n = 0\n","    for data, target in data_source:\n","\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        # apply transform and model on whole batch directly on device\n","        data = transform(data)\n","        output = model(data)\n","\n","        total_loss += target.numel() * F.nll_loss(output.squeeze(), target).item()\n","        n += target.numel()\n","\n","        pred = get_likely_index(output)\n","        correct += number_of_correct(pred, target)\n","    \n"," \n","    accuracy = 100 * correct / len(test_loader.dataset)\n","    \n","    return total_loss / n, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DWRFYunhimLb","outputId":"1659420b-f2fd-4ea4-8f69-d76851548e91","executionInfo":{"status":"ok","timestamp":1650730042351,"user_tz":-120,"elapsed":5279503,"user":{"displayName":"Miriam Martínez","userId":"00485205967731265435"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/84843 (0%)]\tLoss: 3.571424\n","Train Epoch: 1 [5000/84843 (6%)]\tLoss: 3.461487\n","Train Epoch: 1 [10000/84843 (12%)]\tLoss: 3.455849\n","Train Epoch: 1 [15000/84843 (18%)]\tLoss: 3.424867\n","Train Epoch: 1 [20000/84843 (24%)]\tLoss: 3.390561\n","Train Epoch: 1 [25000/84843 (29%)]\tLoss: 3.351603\n","Train Epoch: 1 [30000/84843 (35%)]\tLoss: 3.349901\n","Train Epoch: 1 [35000/84843 (41%)]\tLoss: 3.306721\n","Train Epoch: 1 [40000/84843 (47%)]\tLoss: 3.292545\n","Train Epoch: 1 [45000/84843 (53%)]\tLoss: 3.307273\n","Train Epoch: 1 [50000/84843 (59%)]\tLoss: 3.049288\n","Train Epoch: 1 [55000/84843 (65%)]\tLoss: 2.880633\n","Train Epoch: 1 [60000/84843 (71%)]\tLoss: 3.022099\n","Train Epoch: 1 [65000/84843 (77%)]\tLoss: 2.884846\n","Train Epoch: 1 [70000/84843 (82%)]\tLoss: 2.873397\n","Train Epoch: 1 [75000/84843 (88%)]\tLoss: 2.855606\n","Train Epoch: 1 [80000/84843 (94%)]\tLoss: 2.886472\n","| Validation | val loss  2.74 | val accuracy 21%\n","Train Epoch: 2 [0/84843 (0%)]\tLoss: 2.760344\n","Train Epoch: 2 [5000/84843 (6%)]\tLoss: 2.674073\n","Train Epoch: 2 [10000/84843 (12%)]\tLoss: 2.690248\n","Train Epoch: 2 [15000/84843 (18%)]\tLoss: 2.719734\n","Train Epoch: 2 [20000/84843 (24%)]\tLoss: 2.627761\n","Train Epoch: 2 [25000/84843 (29%)]\tLoss: 2.307661\n","Train Epoch: 2 [30000/84843 (35%)]\tLoss: 2.387936\n","Train Epoch: 2 [35000/84843 (41%)]\tLoss: 2.240852\n","Train Epoch: 2 [40000/84843 (47%)]\tLoss: 2.312826\n","Train Epoch: 2 [45000/84843 (53%)]\tLoss: 2.409923\n","Train Epoch: 2 [50000/84843 (59%)]\tLoss: 2.079832\n","Train Epoch: 2 [55000/84843 (65%)]\tLoss: 2.106418\n","Train Epoch: 2 [60000/84843 (71%)]\tLoss: 2.331620\n","Train Epoch: 2 [65000/84843 (77%)]\tLoss: 1.989001\n","Train Epoch: 2 [70000/84843 (82%)]\tLoss: 1.898004\n","Train Epoch: 2 [75000/84843 (88%)]\tLoss: 1.999209\n","Train Epoch: 2 [80000/84843 (94%)]\tLoss: 2.003329\n","| Validation | val loss  1.84 | val accuracy 45%\n","Train Epoch: 3 [0/84843 (0%)]\tLoss: 1.917567\n","Train Epoch: 3 [5000/84843 (6%)]\tLoss: 1.983233\n","Train Epoch: 3 [10000/84843 (12%)]\tLoss: 1.856061\n","Train Epoch: 3 [15000/84843 (18%)]\tLoss: 1.392169\n","Train Epoch: 3 [20000/84843 (24%)]\tLoss: 1.686516\n","Train Epoch: 3 [25000/84843 (29%)]\tLoss: 1.536713\n","Train Epoch: 3 [30000/84843 (35%)]\tLoss: 1.628062\n","Train Epoch: 3 [35000/84843 (41%)]\tLoss: 1.884428\n","Train Epoch: 3 [40000/84843 (47%)]\tLoss: 1.581593\n","Train Epoch: 3 [45000/84843 (53%)]\tLoss: 1.337750\n","Train Epoch: 3 [50000/84843 (59%)]\tLoss: 1.457530\n","Train Epoch: 3 [55000/84843 (65%)]\tLoss: 1.387177\n","Train Epoch: 3 [60000/84843 (71%)]\tLoss: 1.423532\n","Train Epoch: 3 [65000/84843 (77%)]\tLoss: 1.326026\n","Train Epoch: 3 [70000/84843 (82%)]\tLoss: 1.476500\n","Train Epoch: 3 [75000/84843 (88%)]\tLoss: 1.135535\n","Train Epoch: 3 [80000/84843 (94%)]\tLoss: 1.553739\n","| Validation | val loss  1.29 | val accuracy 59%\n","Train Epoch: 4 [0/84843 (0%)]\tLoss: 1.438908\n","Train Epoch: 4 [5000/84843 (6%)]\tLoss: 1.072598\n","Train Epoch: 4 [10000/84843 (12%)]\tLoss: 1.293183\n","Train Epoch: 4 [15000/84843 (18%)]\tLoss: 1.193820\n","Train Epoch: 4 [20000/84843 (24%)]\tLoss: 1.096503\n","Train Epoch: 4 [25000/84843 (29%)]\tLoss: 1.064037\n","Train Epoch: 4 [30000/84843 (35%)]\tLoss: 1.183532\n","Train Epoch: 4 [35000/84843 (41%)]\tLoss: 1.051490\n","Train Epoch: 4 [40000/84843 (47%)]\tLoss: 1.315642\n","Train Epoch: 4 [45000/84843 (53%)]\tLoss: 1.103495\n","Train Epoch: 4 [50000/84843 (59%)]\tLoss: 1.119476\n","Train Epoch: 4 [55000/84843 (65%)]\tLoss: 1.104827\n","Train Epoch: 4 [60000/84843 (71%)]\tLoss: 1.309544\n","Train Epoch: 4 [65000/84843 (77%)]\tLoss: 0.972016\n","Train Epoch: 4 [70000/84843 (82%)]\tLoss: 1.083973\n","Train Epoch: 4 [75000/84843 (88%)]\tLoss: 0.900833\n","Train Epoch: 4 [80000/84843 (94%)]\tLoss: 0.891405\n","| Validation | val loss  1.04 | val accuracy 65%\n","Train Epoch: 5 [0/84843 (0%)]\tLoss: 1.220899\n","Train Epoch: 5 [5000/84843 (6%)]\tLoss: 1.008440\n","Train Epoch: 5 [10000/84843 (12%)]\tLoss: 0.754938\n","Train Epoch: 5 [15000/84843 (18%)]\tLoss: 0.966490\n","Train Epoch: 5 [20000/84843 (24%)]\tLoss: 0.891925\n","Train Epoch: 5 [25000/84843 (29%)]\tLoss: 0.890777\n","Train Epoch: 5 [30000/84843 (35%)]\tLoss: 0.862850\n","Train Epoch: 5 [35000/84843 (41%)]\tLoss: 1.104527\n","Train Epoch: 5 [40000/84843 (47%)]\tLoss: 0.868853\n","Train Epoch: 5 [45000/84843 (53%)]\tLoss: 1.398182\n","Train Epoch: 5 [50000/84843 (59%)]\tLoss: 1.008736\n","Train Epoch: 5 [55000/84843 (65%)]\tLoss: 0.880681\n","Train Epoch: 5 [60000/84843 (71%)]\tLoss: 1.215683\n","Train Epoch: 5 [65000/84843 (77%)]\tLoss: 1.041334\n","Train Epoch: 5 [70000/84843 (82%)]\tLoss: 0.803574\n","Train Epoch: 5 [75000/84843 (88%)]\tLoss: 0.967440\n","Train Epoch: 5 [80000/84843 (94%)]\tLoss: 0.872103\n","| Validation | val loss  0.78 | val accuracy 71%\n","Train Epoch: 6 [0/84843 (0%)]\tLoss: 0.695618\n","Train Epoch: 6 [5000/84843 (6%)]\tLoss: 0.779326\n","Train Epoch: 6 [10000/84843 (12%)]\tLoss: 0.680966\n","Train Epoch: 6 [15000/84843 (18%)]\tLoss: 0.861273\n","Train Epoch: 6 [20000/84843 (24%)]\tLoss: 0.963443\n","Train Epoch: 6 [25000/84843 (29%)]\tLoss: 0.650891\n","Train Epoch: 6 [30000/84843 (35%)]\tLoss: 0.831136\n","Train Epoch: 6 [35000/84843 (41%)]\tLoss: 0.660723\n","Train Epoch: 6 [40000/84843 (47%)]\tLoss: 0.747061\n","Train Epoch: 6 [45000/84843 (53%)]\tLoss: 0.860825\n","Train Epoch: 6 [50000/84843 (59%)]\tLoss: 0.526675\n","Train Epoch: 6 [55000/84843 (65%)]\tLoss: 0.828676\n","Train Epoch: 6 [60000/84843 (71%)]\tLoss: 0.652341\n","Train Epoch: 6 [65000/84843 (77%)]\tLoss: 0.884770\n","Train Epoch: 6 [70000/84843 (82%)]\tLoss: 0.728338\n","Train Epoch: 6 [75000/84843 (88%)]\tLoss: 1.036628\n","Train Epoch: 6 [80000/84843 (94%)]\tLoss: 0.703158\n","| Validation | val loss  0.66 | val accuracy 75%\n","Train Epoch: 7 [0/84843 (0%)]\tLoss: 0.430627\n","Train Epoch: 7 [5000/84843 (6%)]\tLoss: 0.617747\n","Train Epoch: 7 [10000/84843 (12%)]\tLoss: 0.646210\n","Train Epoch: 7 [15000/84843 (18%)]\tLoss: 0.982245\n","Train Epoch: 7 [20000/84843 (24%)]\tLoss: 1.001281\n","Train Epoch: 7 [25000/84843 (29%)]\tLoss: 0.561926\n","Train Epoch: 7 [30000/84843 (35%)]\tLoss: 0.574241\n","Train Epoch: 7 [35000/84843 (41%)]\tLoss: 0.518769\n","Train Epoch: 7 [40000/84843 (47%)]\tLoss: 0.553447\n","Train Epoch: 7 [45000/84843 (53%)]\tLoss: 0.573921\n","Train Epoch: 7 [50000/84843 (59%)]\tLoss: 0.793277\n","Train Epoch: 7 [55000/84843 (65%)]\tLoss: 0.590159\n","Train Epoch: 7 [60000/84843 (71%)]\tLoss: 0.704668\n","Train Epoch: 7 [65000/84843 (77%)]\tLoss: 0.840850\n","Train Epoch: 7 [70000/84843 (82%)]\tLoss: 0.686943\n","Train Epoch: 7 [75000/84843 (88%)]\tLoss: 0.317802\n","Train Epoch: 7 [80000/84843 (94%)]\tLoss: 0.905448\n","| Validation | val loss  0.61 | val accuracy 75%\n","Train Epoch: 8 [0/84843 (0%)]\tLoss: 0.501257\n","Train Epoch: 8 [5000/84843 (6%)]\tLoss: 0.712633\n","Train Epoch: 8 [10000/84843 (12%)]\tLoss: 0.452241\n","Train Epoch: 8 [15000/84843 (18%)]\tLoss: 0.769734\n","Train Epoch: 8 [20000/84843 (24%)]\tLoss: 0.774140\n","Train Epoch: 8 [25000/84843 (29%)]\tLoss: 0.571312\n","Train Epoch: 8 [30000/84843 (35%)]\tLoss: 0.517260\n","Train Epoch: 8 [35000/84843 (41%)]\tLoss: 0.430238\n","Train Epoch: 8 [40000/84843 (47%)]\tLoss: 0.717981\n","Train Epoch: 8 [45000/84843 (53%)]\tLoss: 0.446854\n","Train Epoch: 8 [50000/84843 (59%)]\tLoss: 0.393450\n","Train Epoch: 8 [55000/84843 (65%)]\tLoss: 0.468063\n","Train Epoch: 8 [60000/84843 (71%)]\tLoss: 0.785171\n","Train Epoch: 8 [65000/84843 (77%)]\tLoss: 0.610989\n","Train Epoch: 8 [70000/84843 (82%)]\tLoss: 0.787421\n","Train Epoch: 8 [75000/84843 (88%)]\tLoss: 0.498036\n","Train Epoch: 8 [80000/84843 (94%)]\tLoss: 0.657548\n","| Validation | val loss  0.55 | val accuracy 77%\n","Train Epoch: 9 [0/84843 (0%)]\tLoss: 0.440862\n","Train Epoch: 9 [5000/84843 (6%)]\tLoss: 0.809619\n","Train Epoch: 9 [10000/84843 (12%)]\tLoss: 0.471216\n","Train Epoch: 9 [15000/84843 (18%)]\tLoss: 0.442173\n","Train Epoch: 9 [20000/84843 (24%)]\tLoss: 0.483706\n","Train Epoch: 9 [25000/84843 (29%)]\tLoss: 0.478550\n","Train Epoch: 9 [30000/84843 (35%)]\tLoss: 0.601775\n","Train Epoch: 9 [35000/84843 (41%)]\tLoss: 0.291006\n","Train Epoch: 9 [40000/84843 (47%)]\tLoss: 0.721522\n","Train Epoch: 9 [45000/84843 (53%)]\tLoss: 0.356716\n","Train Epoch: 9 [50000/84843 (59%)]\tLoss: 0.445631\n","Train Epoch: 9 [55000/84843 (65%)]\tLoss: 0.502531\n","Train Epoch: 9 [60000/84843 (71%)]\tLoss: 0.670269\n","Train Epoch: 9 [65000/84843 (77%)]\tLoss: 0.480084\n","Train Epoch: 9 [70000/84843 (82%)]\tLoss: 0.534377\n","Train Epoch: 9 [75000/84843 (88%)]\tLoss: 0.505833\n","Train Epoch: 9 [80000/84843 (94%)]\tLoss: 0.441260\n","| Validation | val loss  0.52 | val accuracy 77%\n","Train Epoch: 10 [0/84843 (0%)]\tLoss: 0.624728\n","Train Epoch: 10 [5000/84843 (6%)]\tLoss: 0.345005\n","Train Epoch: 10 [10000/84843 (12%)]\tLoss: 0.766961\n","Train Epoch: 10 [15000/84843 (18%)]\tLoss: 0.802857\n","Train Epoch: 10 [20000/84843 (24%)]\tLoss: 0.661604\n","Train Epoch: 10 [25000/84843 (29%)]\tLoss: 0.579305\n","Train Epoch: 10 [30000/84843 (35%)]\tLoss: 0.435099\n","Train Epoch: 10 [35000/84843 (41%)]\tLoss: 0.426853\n","Train Epoch: 10 [40000/84843 (47%)]\tLoss: 0.292486\n","Train Epoch: 10 [45000/84843 (53%)]\tLoss: 0.427267\n","Train Epoch: 10 [50000/84843 (59%)]\tLoss: 0.513163\n","Train Epoch: 10 [55000/84843 (65%)]\tLoss: 0.295753\n","Train Epoch: 10 [60000/84843 (71%)]\tLoss: 0.435359\n","Train Epoch: 10 [65000/84843 (77%)]\tLoss: 0.439889\n","Train Epoch: 10 [70000/84843 (82%)]\tLoss: 0.189493\n","Train Epoch: 10 [75000/84843 (88%)]\tLoss: 0.689332\n","Train Epoch: 10 [80000/84843 (94%)]\tLoss: 0.330436\n","| Validation | val loss  0.48 | val accuracy 78%\n","Train Epoch: 11 [0/84843 (0%)]\tLoss: 0.246550\n","Train Epoch: 11 [5000/84843 (6%)]\tLoss: 0.525417\n","Train Epoch: 11 [10000/84843 (12%)]\tLoss: 0.452119\n","Train Epoch: 11 [15000/84843 (18%)]\tLoss: 0.534806\n","Train Epoch: 11 [20000/84843 (24%)]\tLoss: 0.396985\n","Train Epoch: 11 [25000/84843 (29%)]\tLoss: 0.493713\n","Train Epoch: 11 [30000/84843 (35%)]\tLoss: 0.391624\n","Train Epoch: 11 [35000/84843 (41%)]\tLoss: 0.397778\n","Train Epoch: 11 [40000/84843 (47%)]\tLoss: 0.552943\n","Train Epoch: 11 [45000/84843 (53%)]\tLoss: 0.544777\n","Train Epoch: 11 [50000/84843 (59%)]\tLoss: 0.457788\n","Train Epoch: 11 [55000/84843 (65%)]\tLoss: 0.447071\n","Train Epoch: 11 [60000/84843 (71%)]\tLoss: 0.716633\n","Train Epoch: 11 [65000/84843 (77%)]\tLoss: 0.376579\n","Train Epoch: 11 [70000/84843 (82%)]\tLoss: 0.375439\n","Train Epoch: 11 [75000/84843 (88%)]\tLoss: 0.270694\n","Train Epoch: 11 [80000/84843 (94%)]\tLoss: 0.660996\n","| Validation | val loss  0.44 | val accuracy 79%\n","Train Epoch: 12 [0/84843 (0%)]\tLoss: 0.343662\n","Train Epoch: 12 [5000/84843 (6%)]\tLoss: 0.541020\n","Train Epoch: 12 [10000/84843 (12%)]\tLoss: 0.477399\n","Train Epoch: 12 [15000/84843 (18%)]\tLoss: 0.546842\n","Train Epoch: 12 [20000/84843 (24%)]\tLoss: 0.618795\n","Train Epoch: 12 [25000/84843 (29%)]\tLoss: 0.408480\n","Train Epoch: 12 [30000/84843 (35%)]\tLoss: 0.477471\n","Train Epoch: 12 [35000/84843 (41%)]\tLoss: 0.325505\n","Train Epoch: 12 [40000/84843 (47%)]\tLoss: 0.577084\n","Train Epoch: 12 [45000/84843 (53%)]\tLoss: 0.462089\n","Train Epoch: 12 [50000/84843 (59%)]\tLoss: 0.553593\n","Train Epoch: 12 [55000/84843 (65%)]\tLoss: 0.366173\n","Train Epoch: 12 [60000/84843 (71%)]\tLoss: 0.460584\n","Train Epoch: 12 [65000/84843 (77%)]\tLoss: 0.451008\n","Train Epoch: 12 [70000/84843 (82%)]\tLoss: 0.274967\n","Train Epoch: 12 [75000/84843 (88%)]\tLoss: 0.291673\n","Train Epoch: 12 [80000/84843 (94%)]\tLoss: 0.286471\n","| Validation | val loss  0.44 | val accuracy 79%\n","Train Epoch: 13 [0/84843 (0%)]\tLoss: 0.286297\n","Train Epoch: 13 [5000/84843 (6%)]\tLoss: 0.276531\n","Train Epoch: 13 [10000/84843 (12%)]\tLoss: 0.427436\n","Train Epoch: 13 [15000/84843 (18%)]\tLoss: 0.629455\n","Train Epoch: 13 [20000/84843 (24%)]\tLoss: 0.608426\n","Train Epoch: 13 [25000/84843 (29%)]\tLoss: 0.329353\n","Train Epoch: 13 [30000/84843 (35%)]\tLoss: 0.276934\n","Train Epoch: 13 [35000/84843 (41%)]\tLoss: 0.263272\n","Train Epoch: 13 [40000/84843 (47%)]\tLoss: 0.876430\n","Train Epoch: 13 [45000/84843 (53%)]\tLoss: 0.303125\n","Train Epoch: 13 [50000/84843 (59%)]\tLoss: 0.581520\n","Train Epoch: 13 [55000/84843 (65%)]\tLoss: 0.252760\n","Train Epoch: 13 [60000/84843 (71%)]\tLoss: 0.414388\n","Train Epoch: 13 [65000/84843 (77%)]\tLoss: 0.496301\n","Train Epoch: 13 [70000/84843 (82%)]\tLoss: 0.327089\n","Train Epoch: 13 [75000/84843 (88%)]\tLoss: 0.257411\n","Train Epoch: 13 [80000/84843 (94%)]\tLoss: 0.656090\n","| Validation | val loss  0.39 | val accuracy 80%\n","Train Epoch: 14 [0/84843 (0%)]\tLoss: 0.673750\n","Train Epoch: 14 [5000/84843 (6%)]\tLoss: 0.366285\n","Train Epoch: 14 [10000/84843 (12%)]\tLoss: 0.452437\n","Train Epoch: 14 [15000/84843 (18%)]\tLoss: 0.474229\n","Train Epoch: 14 [20000/84843 (24%)]\tLoss: 0.317944\n","Train Epoch: 14 [25000/84843 (29%)]\tLoss: 0.415777\n","Train Epoch: 14 [30000/84843 (35%)]\tLoss: 0.431712\n","Train Epoch: 14 [35000/84843 (41%)]\tLoss: 0.248939\n","Train Epoch: 14 [40000/84843 (47%)]\tLoss: 0.337225\n","Train Epoch: 14 [45000/84843 (53%)]\tLoss: 0.654593\n","Train Epoch: 14 [50000/84843 (59%)]\tLoss: 0.511578\n","Train Epoch: 14 [55000/84843 (65%)]\tLoss: 0.473372\n","Train Epoch: 14 [60000/84843 (71%)]\tLoss: 0.299908\n","Train Epoch: 14 [65000/84843 (77%)]\tLoss: 0.693683\n","Train Epoch: 14 [70000/84843 (82%)]\tLoss: 0.317025\n","Train Epoch: 14 [75000/84843 (88%)]\tLoss: 0.441428\n","Train Epoch: 14 [80000/84843 (94%)]\tLoss: 0.360553\n","| Validation | val loss  0.38 | val accuracy 81%\n","Train Epoch: 15 [0/84843 (0%)]\tLoss: 0.354965\n","Train Epoch: 15 [5000/84843 (6%)]\tLoss: 0.375434\n","Train Epoch: 15 [10000/84843 (12%)]\tLoss: 0.354576\n","Train Epoch: 15 [15000/84843 (18%)]\tLoss: 0.239087\n","Train Epoch: 15 [20000/84843 (24%)]\tLoss: 0.432389\n","Train Epoch: 15 [25000/84843 (29%)]\tLoss: 0.237161\n","Train Epoch: 15 [30000/84843 (35%)]\tLoss: 0.387886\n","Train Epoch: 15 [35000/84843 (41%)]\tLoss: 0.512424\n","Train Epoch: 15 [40000/84843 (47%)]\tLoss: 0.119556\n","Train Epoch: 15 [45000/84843 (53%)]\tLoss: 0.608465\n","Train Epoch: 15 [50000/84843 (59%)]\tLoss: 0.471022\n","Train Epoch: 15 [55000/84843 (65%)]\tLoss: 0.424440\n","Train Epoch: 15 [60000/84843 (71%)]\tLoss: 0.373258\n","Train Epoch: 15 [65000/84843 (77%)]\tLoss: 0.480304\n","Train Epoch: 15 [70000/84843 (82%)]\tLoss: 0.289609\n","Train Epoch: 15 [75000/84843 (88%)]\tLoss: 0.413176\n","Train Epoch: 15 [80000/84843 (94%)]\tLoss: 0.332045\n","| Validation | val loss  0.40 | val accuracy 80%\n","Train Epoch: 16 [0/84843 (0%)]\tLoss: 0.314721\n","Train Epoch: 16 [5000/84843 (6%)]\tLoss: 0.200952\n","Train Epoch: 16 [10000/84843 (12%)]\tLoss: 0.504132\n","Train Epoch: 16 [15000/84843 (18%)]\tLoss: 0.339497\n","Train Epoch: 16 [20000/84843 (24%)]\tLoss: 0.244345\n","Train Epoch: 16 [25000/84843 (29%)]\tLoss: 0.213143\n","Train Epoch: 16 [30000/84843 (35%)]\tLoss: 0.227532\n","Train Epoch: 16 [35000/84843 (41%)]\tLoss: 0.253503\n","Train Epoch: 16 [40000/84843 (47%)]\tLoss: 0.398951\n","Train Epoch: 16 [45000/84843 (53%)]\tLoss: 0.281616\n","Train Epoch: 16 [50000/84843 (59%)]\tLoss: 0.320361\n","Train Epoch: 16 [55000/84843 (65%)]\tLoss: 0.447391\n","Train Epoch: 16 [60000/84843 (71%)]\tLoss: 0.352089\n","Train Epoch: 16 [65000/84843 (77%)]\tLoss: 0.262212\n","Train Epoch: 16 [70000/84843 (82%)]\tLoss: 0.296792\n","Train Epoch: 16 [75000/84843 (88%)]\tLoss: 0.352865\n","Train Epoch: 16 [80000/84843 (94%)]\tLoss: 0.255209\n","| Validation | val loss  0.39 | val accuracy 80%\n","Train Epoch: 17 [0/84843 (0%)]\tLoss: 0.152111\n","Train Epoch: 17 [5000/84843 (6%)]\tLoss: 0.226672\n","Train Epoch: 17 [10000/84843 (12%)]\tLoss: 0.168066\n","Train Epoch: 17 [15000/84843 (18%)]\tLoss: 0.263132\n","Train Epoch: 17 [20000/84843 (24%)]\tLoss: 0.528034\n","Train Epoch: 17 [25000/84843 (29%)]\tLoss: 0.499580\n","Train Epoch: 17 [30000/84843 (35%)]\tLoss: 0.324817\n","Train Epoch: 17 [35000/84843 (41%)]\tLoss: 0.295570\n","Train Epoch: 17 [40000/84843 (47%)]\tLoss: 0.234818\n","Train Epoch: 17 [45000/84843 (53%)]\tLoss: 0.315816\n","Train Epoch: 17 [50000/84843 (59%)]\tLoss: 0.499129\n","Train Epoch: 17 [55000/84843 (65%)]\tLoss: 0.341072\n","Train Epoch: 17 [60000/84843 (71%)]\tLoss: 0.461827\n","Train Epoch: 17 [65000/84843 (77%)]\tLoss: 0.212895\n","Train Epoch: 17 [70000/84843 (82%)]\tLoss: 0.240000\n","Train Epoch: 17 [75000/84843 (88%)]\tLoss: 0.443011\n","Train Epoch: 17 [80000/84843 (94%)]\tLoss: 0.310074\n","| Validation | val loss  0.37 | val accuracy 81%\n","Train Epoch: 18 [0/84843 (0%)]\tLoss: 0.333276\n","Train Epoch: 18 [5000/84843 (6%)]\tLoss: 0.495350\n","Train Epoch: 18 [10000/84843 (12%)]\tLoss: 0.215136\n","Train Epoch: 18 [15000/84843 (18%)]\tLoss: 0.445855\n","Train Epoch: 18 [20000/84843 (24%)]\tLoss: 0.344291\n","Train Epoch: 18 [25000/84843 (29%)]\tLoss: 0.433483\n","Train Epoch: 18 [30000/84843 (35%)]\tLoss: 0.379556\n","Train Epoch: 18 [35000/84843 (41%)]\tLoss: 0.215143\n","Train Epoch: 18 [40000/84843 (47%)]\tLoss: 0.303292\n","Train Epoch: 18 [45000/84843 (53%)]\tLoss: 0.274957\n","Train Epoch: 18 [50000/84843 (59%)]\tLoss: 0.261971\n","Train Epoch: 18 [55000/84843 (65%)]\tLoss: 0.123499\n","Train Epoch: 18 [60000/84843 (71%)]\tLoss: 0.439160\n","Train Epoch: 18 [65000/84843 (77%)]\tLoss: 0.360068\n","Train Epoch: 18 [70000/84843 (82%)]\tLoss: 0.204360\n","Train Epoch: 18 [75000/84843 (88%)]\tLoss: 0.329313\n","Train Epoch: 18 [80000/84843 (94%)]\tLoss: 0.281369\n","| Validation | val loss  0.36 | val accuracy 82%\n","Train Epoch: 19 [0/84843 (0%)]\tLoss: 0.280866\n","Train Epoch: 19 [5000/84843 (6%)]\tLoss: 0.249995\n","Train Epoch: 19 [10000/84843 (12%)]\tLoss: 0.261324\n","Train Epoch: 19 [15000/84843 (18%)]\tLoss: 0.254764\n","Train Epoch: 19 [20000/84843 (24%)]\tLoss: 0.455440\n","Train Epoch: 19 [25000/84843 (29%)]\tLoss: 0.365578\n","Train Epoch: 19 [30000/84843 (35%)]\tLoss: 0.108820\n","Train Epoch: 19 [35000/84843 (41%)]\tLoss: 0.259926\n","Train Epoch: 19 [40000/84843 (47%)]\tLoss: 0.333219\n","Train Epoch: 19 [45000/84843 (53%)]\tLoss: 0.249443\n","Train Epoch: 19 [50000/84843 (59%)]\tLoss: 0.194051\n","Train Epoch: 19 [55000/84843 (65%)]\tLoss: 0.432978\n","Train Epoch: 19 [60000/84843 (71%)]\tLoss: 0.289638\n","Train Epoch: 19 [65000/84843 (77%)]\tLoss: 0.423084\n","Train Epoch: 19 [70000/84843 (82%)]\tLoss: 0.350762\n","Train Epoch: 19 [75000/84843 (88%)]\tLoss: 0.270416\n","Train Epoch: 19 [80000/84843 (94%)]\tLoss: 0.141248\n","| Validation | val loss  0.34 | val accuracy 82%\n","Train Epoch: 20 [0/84843 (0%)]\tLoss: 0.194968\n","Train Epoch: 20 [5000/84843 (6%)]\tLoss: 0.292941\n","Train Epoch: 20 [10000/84843 (12%)]\tLoss: 0.218536\n","Train Epoch: 20 [15000/84843 (18%)]\tLoss: 0.374254\n","Train Epoch: 20 [20000/84843 (24%)]\tLoss: 0.136845\n","Train Epoch: 20 [25000/84843 (29%)]\tLoss: 0.238467\n","Train Epoch: 20 [30000/84843 (35%)]\tLoss: 0.251000\n","Train Epoch: 20 [35000/84843 (41%)]\tLoss: 0.121236\n","Train Epoch: 20 [40000/84843 (47%)]\tLoss: 0.419256\n","Train Epoch: 20 [45000/84843 (53%)]\tLoss: 0.277829\n","Train Epoch: 20 [50000/84843 (59%)]\tLoss: 0.367637\n","Train Epoch: 20 [55000/84843 (65%)]\tLoss: 0.212892\n","Train Epoch: 20 [60000/84843 (71%)]\tLoss: 0.212382\n","Train Epoch: 20 [65000/84843 (77%)]\tLoss: 0.208269\n","Train Epoch: 20 [70000/84843 (82%)]\tLoss: 0.445716\n","Train Epoch: 20 [75000/84843 (88%)]\tLoss: 0.230771\n","Train Epoch: 20 [80000/84843 (94%)]\tLoss: 0.201005\n","| Validation | val loss  0.37 | val accuracy 81%\n","Train Epoch: 21 [0/84843 (0%)]\tLoss: 0.368784\n","Train Epoch: 21 [5000/84843 (6%)]\tLoss: 0.175225\n","Train Epoch: 21 [10000/84843 (12%)]\tLoss: 0.287705\n","Train Epoch: 21 [15000/84843 (18%)]\tLoss: 0.174250\n","Train Epoch: 21 [20000/84843 (24%)]\tLoss: 0.102421\n","Train Epoch: 21 [25000/84843 (29%)]\tLoss: 0.098030\n","Train Epoch: 21 [30000/84843 (35%)]\tLoss: 0.140243\n","Train Epoch: 21 [35000/84843 (41%)]\tLoss: 0.220356\n","Train Epoch: 21 [40000/84843 (47%)]\tLoss: 0.378641\n","Train Epoch: 21 [45000/84843 (53%)]\tLoss: 0.287943\n","Train Epoch: 21 [50000/84843 (59%)]\tLoss: 0.391091\n","Train Epoch: 21 [55000/84843 (65%)]\tLoss: 0.095993\n","Train Epoch: 21 [60000/84843 (71%)]\tLoss: 0.197571\n","Train Epoch: 21 [65000/84843 (77%)]\tLoss: 0.357685\n","Train Epoch: 21 [70000/84843 (82%)]\tLoss: 0.240272\n","Train Epoch: 21 [75000/84843 (88%)]\tLoss: 0.318590\n","Train Epoch: 21 [80000/84843 (94%)]\tLoss: 0.208271\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 22 [0/84843 (0%)]\tLoss: 0.116589\n","Train Epoch: 22 [5000/84843 (6%)]\tLoss: 0.159431\n","Train Epoch: 22 [10000/84843 (12%)]\tLoss: 0.215306\n","Train Epoch: 22 [15000/84843 (18%)]\tLoss: 0.207718\n","Train Epoch: 22 [20000/84843 (24%)]\tLoss: 0.057026\n","Train Epoch: 22 [25000/84843 (29%)]\tLoss: 0.298168\n","Train Epoch: 22 [30000/84843 (35%)]\tLoss: 0.187049\n","Train Epoch: 22 [35000/84843 (41%)]\tLoss: 0.120548\n","Train Epoch: 22 [40000/84843 (47%)]\tLoss: 0.100416\n","Train Epoch: 22 [45000/84843 (53%)]\tLoss: 0.400308\n","Train Epoch: 22 [50000/84843 (59%)]\tLoss: 0.248198\n","Train Epoch: 22 [55000/84843 (65%)]\tLoss: 0.111895\n","Train Epoch: 22 [60000/84843 (71%)]\tLoss: 0.177895\n","Train Epoch: 22 [65000/84843 (77%)]\tLoss: 0.403299\n","Train Epoch: 22 [70000/84843 (82%)]\tLoss: 0.277310\n","Train Epoch: 22 [75000/84843 (88%)]\tLoss: 0.331817\n","Train Epoch: 22 [80000/84843 (94%)]\tLoss: 0.166078\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 23 [0/84843 (0%)]\tLoss: 0.140595\n","Train Epoch: 23 [5000/84843 (6%)]\tLoss: 0.157595\n","Train Epoch: 23 [10000/84843 (12%)]\tLoss: 0.128784\n","Train Epoch: 23 [15000/84843 (18%)]\tLoss: 0.195710\n","Train Epoch: 23 [20000/84843 (24%)]\tLoss: 0.122486\n","Train Epoch: 23 [25000/84843 (29%)]\tLoss: 0.233337\n","Train Epoch: 23 [30000/84843 (35%)]\tLoss: 0.291585\n","Train Epoch: 23 [35000/84843 (41%)]\tLoss: 0.233712\n","Train Epoch: 23 [40000/84843 (47%)]\tLoss: 0.285472\n","Train Epoch: 23 [45000/84843 (53%)]\tLoss: 0.256248\n","Train Epoch: 23 [50000/84843 (59%)]\tLoss: 0.131046\n","Train Epoch: 23 [55000/84843 (65%)]\tLoss: 0.306740\n","Train Epoch: 23 [60000/84843 (71%)]\tLoss: 0.258494\n","Train Epoch: 23 [65000/84843 (77%)]\tLoss: 0.139770\n","Train Epoch: 23 [70000/84843 (82%)]\tLoss: 0.218197\n","Train Epoch: 23 [75000/84843 (88%)]\tLoss: 0.277959\n","Train Epoch: 23 [80000/84843 (94%)]\tLoss: 0.081068\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 24 [0/84843 (0%)]\tLoss: 0.146952\n","Train Epoch: 24 [5000/84843 (6%)]\tLoss: 0.107660\n","Train Epoch: 24 [10000/84843 (12%)]\tLoss: 0.256187\n","Train Epoch: 24 [15000/84843 (18%)]\tLoss: 0.131559\n","Train Epoch: 24 [20000/84843 (24%)]\tLoss: 0.176132\n","Train Epoch: 24 [25000/84843 (29%)]\tLoss: 0.165005\n","Train Epoch: 24 [30000/84843 (35%)]\tLoss: 0.162561\n","Train Epoch: 24 [35000/84843 (41%)]\tLoss: 0.154685\n","Train Epoch: 24 [40000/84843 (47%)]\tLoss: 0.143246\n","Train Epoch: 24 [45000/84843 (53%)]\tLoss: 0.080300\n","Train Epoch: 24 [50000/84843 (59%)]\tLoss: 0.281993\n","Train Epoch: 24 [55000/84843 (65%)]\tLoss: 0.158699\n","Train Epoch: 24 [60000/84843 (71%)]\tLoss: 0.053122\n","Train Epoch: 24 [65000/84843 (77%)]\tLoss: 0.170226\n","Train Epoch: 24 [70000/84843 (82%)]\tLoss: 0.101279\n","Train Epoch: 24 [75000/84843 (88%)]\tLoss: 0.250619\n","Train Epoch: 24 [80000/84843 (94%)]\tLoss: 0.110219\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 25 [0/84843 (0%)]\tLoss: 0.321901\n","Train Epoch: 25 [5000/84843 (6%)]\tLoss: 0.195430\n","Train Epoch: 25 [10000/84843 (12%)]\tLoss: 0.235821\n","Train Epoch: 25 [15000/84843 (18%)]\tLoss: 0.184114\n","Train Epoch: 25 [20000/84843 (24%)]\tLoss: 0.599987\n","Train Epoch: 25 [25000/84843 (29%)]\tLoss: 0.142776\n","Train Epoch: 25 [30000/84843 (35%)]\tLoss: 0.203277\n","Train Epoch: 25 [35000/84843 (41%)]\tLoss: 0.070460\n","Train Epoch: 25 [40000/84843 (47%)]\tLoss: 0.160162\n","Train Epoch: 25 [45000/84843 (53%)]\tLoss: 0.190936\n","Train Epoch: 25 [50000/84843 (59%)]\tLoss: 0.239894\n","Train Epoch: 25 [55000/84843 (65%)]\tLoss: 0.050472\n","Train Epoch: 25 [60000/84843 (71%)]\tLoss: 0.137553\n","Train Epoch: 25 [65000/84843 (77%)]\tLoss: 0.268105\n","Train Epoch: 25 [70000/84843 (82%)]\tLoss: 0.270049\n","Train Epoch: 25 [75000/84843 (88%)]\tLoss: 0.233064\n","Train Epoch: 25 [80000/84843 (94%)]\tLoss: 0.197706\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 26 [0/84843 (0%)]\tLoss: 0.132748\n","Train Epoch: 26 [5000/84843 (6%)]\tLoss: 0.151124\n","Train Epoch: 26 [10000/84843 (12%)]\tLoss: 0.238015\n","Train Epoch: 26 [15000/84843 (18%)]\tLoss: 0.120808\n","Train Epoch: 26 [20000/84843 (24%)]\tLoss: 0.167773\n","Train Epoch: 26 [25000/84843 (29%)]\tLoss: 0.361210\n","Train Epoch: 26 [30000/84843 (35%)]\tLoss: 0.341796\n","Train Epoch: 26 [35000/84843 (41%)]\tLoss: 0.345646\n","Train Epoch: 26 [40000/84843 (47%)]\tLoss: 0.159135\n","Train Epoch: 26 [45000/84843 (53%)]\tLoss: 0.301021\n","Train Epoch: 26 [50000/84843 (59%)]\tLoss: 0.220869\n","Train Epoch: 26 [55000/84843 (65%)]\tLoss: 0.422661\n","Train Epoch: 26 [60000/84843 (71%)]\tLoss: 0.124835\n","Train Epoch: 26 [65000/84843 (77%)]\tLoss: 0.300131\n","Train Epoch: 26 [70000/84843 (82%)]\tLoss: 0.099520\n","Train Epoch: 26 [75000/84843 (88%)]\tLoss: 0.223166\n","Train Epoch: 26 [80000/84843 (94%)]\tLoss: 0.299935\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 27 [0/84843 (0%)]\tLoss: 0.136773\n","Train Epoch: 27 [5000/84843 (6%)]\tLoss: 0.269207\n","Train Epoch: 27 [10000/84843 (12%)]\tLoss: 0.156521\n","Train Epoch: 27 [15000/84843 (18%)]\tLoss: 0.107864\n","Train Epoch: 27 [20000/84843 (24%)]\tLoss: 0.123410\n","Train Epoch: 27 [25000/84843 (29%)]\tLoss: 0.206132\n","Train Epoch: 27 [30000/84843 (35%)]\tLoss: 0.339713\n","Train Epoch: 27 [35000/84843 (41%)]\tLoss: 0.156994\n","Train Epoch: 27 [40000/84843 (47%)]\tLoss: 0.156476\n","Train Epoch: 27 [45000/84843 (53%)]\tLoss: 0.396121\n","Train Epoch: 27 [50000/84843 (59%)]\tLoss: 0.093706\n","Train Epoch: 27 [55000/84843 (65%)]\tLoss: 0.151976\n","Train Epoch: 27 [60000/84843 (71%)]\tLoss: 0.190483\n","Train Epoch: 27 [65000/84843 (77%)]\tLoss: 0.249443\n","Train Epoch: 27 [70000/84843 (82%)]\tLoss: 0.211619\n","Train Epoch: 27 [75000/84843 (88%)]\tLoss: 0.208070\n","Train Epoch: 27 [80000/84843 (94%)]\tLoss: 0.097736\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 28 [0/84843 (0%)]\tLoss: 0.301843\n","Train Epoch: 28 [5000/84843 (6%)]\tLoss: 0.223254\n","Train Epoch: 28 [10000/84843 (12%)]\tLoss: 0.210714\n","Train Epoch: 28 [15000/84843 (18%)]\tLoss: 0.353128\n","Train Epoch: 28 [20000/84843 (24%)]\tLoss: 0.266535\n","Train Epoch: 28 [25000/84843 (29%)]\tLoss: 0.250186\n","Train Epoch: 28 [30000/84843 (35%)]\tLoss: 0.202342\n","Train Epoch: 28 [35000/84843 (41%)]\tLoss: 0.310919\n","Train Epoch: 28 [40000/84843 (47%)]\tLoss: 0.341251\n","Train Epoch: 28 [45000/84843 (53%)]\tLoss: 0.138736\n","Train Epoch: 28 [50000/84843 (59%)]\tLoss: 0.142848\n","Train Epoch: 28 [55000/84843 (65%)]\tLoss: 0.295582\n","Train Epoch: 28 [60000/84843 (71%)]\tLoss: 0.205606\n","Train Epoch: 28 [65000/84843 (77%)]\tLoss: 0.134472\n","Train Epoch: 28 [70000/84843 (82%)]\tLoss: 0.137813\n","Train Epoch: 28 [75000/84843 (88%)]\tLoss: 0.246947\n","Train Epoch: 28 [80000/84843 (94%)]\tLoss: 0.324111\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 29 [0/84843 (0%)]\tLoss: 0.328963\n","Train Epoch: 29 [5000/84843 (6%)]\tLoss: 0.374491\n","Train Epoch: 29 [10000/84843 (12%)]\tLoss: 0.167850\n","Train Epoch: 29 [15000/84843 (18%)]\tLoss: 0.059153\n","Train Epoch: 29 [20000/84843 (24%)]\tLoss: 0.329475\n","Train Epoch: 29 [25000/84843 (29%)]\tLoss: 0.294011\n","Train Epoch: 29 [30000/84843 (35%)]\tLoss: 0.206556\n","Train Epoch: 29 [35000/84843 (41%)]\tLoss: 0.241613\n","Train Epoch: 29 [40000/84843 (47%)]\tLoss: 0.238485\n","Train Epoch: 29 [45000/84843 (53%)]\tLoss: 0.118912\n","Train Epoch: 29 [50000/84843 (59%)]\tLoss: 0.041425\n","Train Epoch: 29 [55000/84843 (65%)]\tLoss: 0.173102\n","Train Epoch: 29 [60000/84843 (71%)]\tLoss: 0.149850\n","Train Epoch: 29 [65000/84843 (77%)]\tLoss: 0.269738\n","Train Epoch: 29 [70000/84843 (82%)]\tLoss: 0.150327\n","Train Epoch: 29 [75000/84843 (88%)]\tLoss: 0.137133\n","Train Epoch: 29 [80000/84843 (94%)]\tLoss: 0.236466\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 30 [0/84843 (0%)]\tLoss: 0.251340\n","Train Epoch: 30 [5000/84843 (6%)]\tLoss: 0.111208\n","Train Epoch: 30 [10000/84843 (12%)]\tLoss: 0.209208\n","Train Epoch: 30 [15000/84843 (18%)]\tLoss: 0.153420\n","Train Epoch: 30 [20000/84843 (24%)]\tLoss: 0.201208\n","Train Epoch: 30 [25000/84843 (29%)]\tLoss: 0.283404\n","Train Epoch: 30 [30000/84843 (35%)]\tLoss: 0.129913\n","Train Epoch: 30 [35000/84843 (41%)]\tLoss: 0.186471\n","Train Epoch: 30 [40000/84843 (47%)]\tLoss: 0.195821\n","Train Epoch: 30 [45000/84843 (53%)]\tLoss: 0.277176\n","Train Epoch: 30 [50000/84843 (59%)]\tLoss: 0.120219\n","Train Epoch: 30 [55000/84843 (65%)]\tLoss: 0.087779\n","Train Epoch: 30 [60000/84843 (71%)]\tLoss: 0.270755\n","Train Epoch: 30 [65000/84843 (77%)]\tLoss: 0.116027\n","Train Epoch: 30 [70000/84843 (82%)]\tLoss: 0.227223\n","Train Epoch: 30 [75000/84843 (88%)]\tLoss: 0.279424\n","Train Epoch: 30 [80000/84843 (94%)]\tLoss: 0.231620\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 31 [0/84843 (0%)]\tLoss: 0.098441\n","Train Epoch: 31 [5000/84843 (6%)]\tLoss: 0.233812\n","Train Epoch: 31 [10000/84843 (12%)]\tLoss: 0.183958\n","Train Epoch: 31 [15000/84843 (18%)]\tLoss: 0.075358\n","Train Epoch: 31 [20000/84843 (24%)]\tLoss: 0.316805\n","Train Epoch: 31 [25000/84843 (29%)]\tLoss: 0.139765\n","Train Epoch: 31 [30000/84843 (35%)]\tLoss: 0.359865\n","Train Epoch: 31 [35000/84843 (41%)]\tLoss: 0.176895\n","Train Epoch: 31 [40000/84843 (47%)]\tLoss: 0.162325\n","Train Epoch: 31 [45000/84843 (53%)]\tLoss: 0.122538\n","Train Epoch: 31 [50000/84843 (59%)]\tLoss: 0.178593\n","Train Epoch: 31 [55000/84843 (65%)]\tLoss: 0.108113\n","Train Epoch: 31 [60000/84843 (71%)]\tLoss: 0.029748\n","Train Epoch: 31 [65000/84843 (77%)]\tLoss: 0.221493\n","Train Epoch: 31 [70000/84843 (82%)]\tLoss: 0.314961\n","Train Epoch: 31 [75000/84843 (88%)]\tLoss: 0.118754\n","Train Epoch: 31 [80000/84843 (94%)]\tLoss: 0.180561\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 32 [0/84843 (0%)]\tLoss: 0.146249\n","Train Epoch: 32 [5000/84843 (6%)]\tLoss: 0.220770\n","Train Epoch: 32 [10000/84843 (12%)]\tLoss: 0.147914\n","Train Epoch: 32 [15000/84843 (18%)]\tLoss: 0.263564\n","Train Epoch: 32 [20000/84843 (24%)]\tLoss: 0.061566\n","Train Epoch: 32 [25000/84843 (29%)]\tLoss: 0.041339\n","Train Epoch: 32 [30000/84843 (35%)]\tLoss: 0.146206\n","Train Epoch: 32 [35000/84843 (41%)]\tLoss: 0.374951\n","Train Epoch: 32 [40000/84843 (47%)]\tLoss: 0.043621\n","Train Epoch: 32 [45000/84843 (53%)]\tLoss: 0.071843\n","Train Epoch: 32 [50000/84843 (59%)]\tLoss: 0.223300\n","Train Epoch: 32 [55000/84843 (65%)]\tLoss: 0.101343\n","Train Epoch: 32 [60000/84843 (71%)]\tLoss: 0.168619\n","Train Epoch: 32 [65000/84843 (77%)]\tLoss: 0.250699\n","Train Epoch: 32 [70000/84843 (82%)]\tLoss: 0.138792\n","Train Epoch: 32 [75000/84843 (88%)]\tLoss: 0.122157\n","Train Epoch: 32 [80000/84843 (94%)]\tLoss: 0.278767\n","| Validation | val loss  0.29 | val accuracy 83%\n","Train Epoch: 33 [0/84843 (0%)]\tLoss: 0.272231\n","Train Epoch: 33 [5000/84843 (6%)]\tLoss: 0.398338\n","Train Epoch: 33 [10000/84843 (12%)]\tLoss: 0.092547\n","Train Epoch: 33 [15000/84843 (18%)]\tLoss: 0.188458\n","Train Epoch: 33 [20000/84843 (24%)]\tLoss: 0.393713\n","Train Epoch: 33 [25000/84843 (29%)]\tLoss: 0.118592\n","Train Epoch: 33 [30000/84843 (35%)]\tLoss: 0.189696\n","Train Epoch: 33 [35000/84843 (41%)]\tLoss: 0.316660\n","Train Epoch: 33 [40000/84843 (47%)]\tLoss: 0.310580\n","Train Epoch: 33 [45000/84843 (53%)]\tLoss: 0.177051\n","Train Epoch: 33 [50000/84843 (59%)]\tLoss: 0.132689\n","Train Epoch: 33 [55000/84843 (65%)]\tLoss: 0.065221\n","Train Epoch: 33 [60000/84843 (71%)]\tLoss: 0.118221\n","Train Epoch: 33 [65000/84843 (77%)]\tLoss: 0.112042\n","Train Epoch: 33 [70000/84843 (82%)]\tLoss: 0.177634\n","Train Epoch: 33 [75000/84843 (88%)]\tLoss: 0.337481\n","Train Epoch: 33 [80000/84843 (94%)]\tLoss: 0.226258\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 34 [0/84843 (0%)]\tLoss: 0.103561\n","Train Epoch: 34 [5000/84843 (6%)]\tLoss: 0.189567\n","Train Epoch: 34 [10000/84843 (12%)]\tLoss: 0.137578\n","Train Epoch: 34 [15000/84843 (18%)]\tLoss: 0.069218\n","Train Epoch: 34 [20000/84843 (24%)]\tLoss: 0.096472\n","Train Epoch: 34 [25000/84843 (29%)]\tLoss: 0.257411\n","Train Epoch: 34 [30000/84843 (35%)]\tLoss: 0.111400\n","Train Epoch: 34 [35000/84843 (41%)]\tLoss: 0.380127\n","Train Epoch: 34 [40000/84843 (47%)]\tLoss: 0.163326\n","Train Epoch: 34 [45000/84843 (53%)]\tLoss: 0.102268\n","Train Epoch: 34 [50000/84843 (59%)]\tLoss: 0.088366\n","Train Epoch: 34 [55000/84843 (65%)]\tLoss: 0.199096\n","Train Epoch: 34 [60000/84843 (71%)]\tLoss: 0.173626\n","Train Epoch: 34 [65000/84843 (77%)]\tLoss: 0.269854\n","Train Epoch: 34 [70000/84843 (82%)]\tLoss: 0.169052\n","Train Epoch: 34 [75000/84843 (88%)]\tLoss: 0.135549\n","Train Epoch: 34 [80000/84843 (94%)]\tLoss: 0.091452\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 35 [0/84843 (0%)]\tLoss: 0.055638\n","Train Epoch: 35 [5000/84843 (6%)]\tLoss: 0.254264\n","Train Epoch: 35 [10000/84843 (12%)]\tLoss: 0.298103\n","Train Epoch: 35 [15000/84843 (18%)]\tLoss: 0.198616\n","Train Epoch: 35 [20000/84843 (24%)]\tLoss: 0.129056\n","Train Epoch: 35 [25000/84843 (29%)]\tLoss: 0.192886\n","Train Epoch: 35 [30000/84843 (35%)]\tLoss: 0.254641\n","Train Epoch: 35 [35000/84843 (41%)]\tLoss: 0.204691\n","Train Epoch: 35 [40000/84843 (47%)]\tLoss: 0.235425\n","Train Epoch: 35 [45000/84843 (53%)]\tLoss: 0.166939\n","Train Epoch: 35 [50000/84843 (59%)]\tLoss: 0.083879\n","Train Epoch: 35 [55000/84843 (65%)]\tLoss: 0.195646\n","Train Epoch: 35 [60000/84843 (71%)]\tLoss: 0.194488\n","Train Epoch: 35 [65000/84843 (77%)]\tLoss: 0.230508\n","Train Epoch: 35 [70000/84843 (82%)]\tLoss: 0.201023\n","Train Epoch: 35 [75000/84843 (88%)]\tLoss: 0.171982\n","Train Epoch: 35 [80000/84843 (94%)]\tLoss: 0.232028\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 36 [0/84843 (0%)]\tLoss: 0.083080\n","Train Epoch: 36 [5000/84843 (6%)]\tLoss: 0.158846\n","Train Epoch: 36 [10000/84843 (12%)]\tLoss: 0.281698\n","Train Epoch: 36 [15000/84843 (18%)]\tLoss: 0.141587\n","Train Epoch: 36 [20000/84843 (24%)]\tLoss: 0.242898\n","Train Epoch: 36 [25000/84843 (29%)]\tLoss: 0.118672\n","Train Epoch: 36 [30000/84843 (35%)]\tLoss: 0.067512\n","Train Epoch: 36 [35000/84843 (41%)]\tLoss: 0.183459\n","Train Epoch: 36 [40000/84843 (47%)]\tLoss: 0.244163\n","Train Epoch: 36 [45000/84843 (53%)]\tLoss: 0.210493\n","Train Epoch: 36 [50000/84843 (59%)]\tLoss: 0.312238\n","Train Epoch: 36 [55000/84843 (65%)]\tLoss: 0.065269\n","Train Epoch: 36 [60000/84843 (71%)]\tLoss: 0.236106\n","Train Epoch: 36 [65000/84843 (77%)]\tLoss: 0.112210\n","Train Epoch: 36 [70000/84843 (82%)]\tLoss: 0.156803\n","Train Epoch: 36 [75000/84843 (88%)]\tLoss: 0.241535\n","Train Epoch: 36 [80000/84843 (94%)]\tLoss: 0.180053\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 37 [0/84843 (0%)]\tLoss: 0.104785\n","Train Epoch: 37 [5000/84843 (6%)]\tLoss: 0.168404\n","Train Epoch: 37 [10000/84843 (12%)]\tLoss: 0.140892\n","Train Epoch: 37 [15000/84843 (18%)]\tLoss: 0.176467\n","Train Epoch: 37 [20000/84843 (24%)]\tLoss: 0.189310\n","Train Epoch: 37 [25000/84843 (29%)]\tLoss: 0.214299\n","Train Epoch: 37 [30000/84843 (35%)]\tLoss: 0.243633\n","Train Epoch: 37 [35000/84843 (41%)]\tLoss: 0.202683\n","Train Epoch: 37 [40000/84843 (47%)]\tLoss: 0.154414\n","Train Epoch: 37 [45000/84843 (53%)]\tLoss: 0.254260\n","Train Epoch: 37 [50000/84843 (59%)]\tLoss: 0.253784\n","Train Epoch: 37 [55000/84843 (65%)]\tLoss: 0.288689\n","Train Epoch: 37 [60000/84843 (71%)]\tLoss: 0.249162\n","Train Epoch: 37 [65000/84843 (77%)]\tLoss: 0.137362\n","Train Epoch: 37 [70000/84843 (82%)]\tLoss: 0.314087\n","Train Epoch: 37 [75000/84843 (88%)]\tLoss: 0.156322\n","Train Epoch: 37 [80000/84843 (94%)]\tLoss: 0.235459\n","| Validation | val loss  0.30 | val accuracy 83%\n","Train Epoch: 38 [0/84843 (0%)]\tLoss: 0.202367\n","Train Epoch: 38 [5000/84843 (6%)]\tLoss: 0.053230\n","Train Epoch: 38 [10000/84843 (12%)]\tLoss: 0.102569\n","Train Epoch: 38 [15000/84843 (18%)]\tLoss: 0.285234\n","Train Epoch: 38 [20000/84843 (24%)]\tLoss: 0.192992\n","Train Epoch: 38 [25000/84843 (29%)]\tLoss: 0.133879\n","Train Epoch: 38 [30000/84843 (35%)]\tLoss: 0.084802\n","Train Epoch: 38 [35000/84843 (41%)]\tLoss: 0.342861\n","Train Epoch: 38 [40000/84843 (47%)]\tLoss: 0.150210\n","Train Epoch: 38 [45000/84843 (53%)]\tLoss: 0.178932\n","Train Epoch: 38 [50000/84843 (59%)]\tLoss: 0.117775\n","Train Epoch: 38 [55000/84843 (65%)]\tLoss: 0.314732\n","Train Epoch: 38 [60000/84843 (71%)]\tLoss: 0.117176\n","Train Epoch: 38 [65000/84843 (77%)]\tLoss: 0.257900\n","Train Epoch: 38 [70000/84843 (82%)]\tLoss: 0.142506\n","Train Epoch: 38 [75000/84843 (88%)]\tLoss: 0.283231\n","Train Epoch: 38 [80000/84843 (94%)]\tLoss: 0.393290\n","| Validation | val loss  0.29 | val accuracy 83%\n","Train Epoch: 39 [0/84843 (0%)]\tLoss: 0.210528\n","Train Epoch: 39 [5000/84843 (6%)]\tLoss: 0.149591\n","Train Epoch: 39 [10000/84843 (12%)]\tLoss: 0.168967\n","Train Epoch: 39 [15000/84843 (18%)]\tLoss: 0.307647\n","Train Epoch: 39 [20000/84843 (24%)]\tLoss: 0.356651\n","Train Epoch: 39 [25000/84843 (29%)]\tLoss: 0.161815\n","Train Epoch: 39 [30000/84843 (35%)]\tLoss: 0.182823\n","Train Epoch: 39 [35000/84843 (41%)]\tLoss: 0.156476\n","Train Epoch: 39 [40000/84843 (47%)]\tLoss: 0.252921\n","Train Epoch: 39 [45000/84843 (53%)]\tLoss: 0.067177\n","Train Epoch: 39 [50000/84843 (59%)]\tLoss: 0.205407\n","Train Epoch: 39 [55000/84843 (65%)]\tLoss: 0.132720\n","Train Epoch: 39 [60000/84843 (71%)]\tLoss: 0.217346\n","Train Epoch: 39 [65000/84843 (77%)]\tLoss: 0.147840\n","Train Epoch: 39 [70000/84843 (82%)]\tLoss: 0.159090\n","Train Epoch: 39 [75000/84843 (88%)]\tLoss: 0.219628\n","Train Epoch: 39 [80000/84843 (94%)]\tLoss: 0.120344\n","| Validation | val loss  0.29 | val accuracy 83%\n","Train Epoch: 40 [0/84843 (0%)]\tLoss: 0.281298\n","Train Epoch: 40 [5000/84843 (6%)]\tLoss: 0.255953\n","Train Epoch: 40 [10000/84843 (12%)]\tLoss: 0.107917\n","Train Epoch: 40 [15000/84843 (18%)]\tLoss: 0.220237\n","Train Epoch: 40 [20000/84843 (24%)]\tLoss: 0.173818\n","Train Epoch: 40 [25000/84843 (29%)]\tLoss: 0.176123\n","Train Epoch: 40 [30000/84843 (35%)]\tLoss: 0.130129\n","Train Epoch: 40 [35000/84843 (41%)]\tLoss: 0.077209\n","Train Epoch: 40 [40000/84843 (47%)]\tLoss: 0.296282\n","Train Epoch: 40 [45000/84843 (53%)]\tLoss: 0.338586\n","Train Epoch: 40 [50000/84843 (59%)]\tLoss: 0.256187\n","Train Epoch: 40 [55000/84843 (65%)]\tLoss: 0.139475\n","Train Epoch: 40 [60000/84843 (71%)]\tLoss: 0.086777\n","Train Epoch: 40 [65000/84843 (77%)]\tLoss: 0.403588\n","Train Epoch: 40 [70000/84843 (82%)]\tLoss: 0.094482\n","Train Epoch: 40 [75000/84843 (88%)]\tLoss: 0.303710\n","Train Epoch: 40 [80000/84843 (94%)]\tLoss: 0.230284\n","| Validation | val loss  0.30 | val accuracy 83%\n","=========================================================================================\n","| End of training | test loss  0.34 | test accuracy 90%\n","=========================================================================================\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'training loss')"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["best_val_loss = float(\"inf\")\n","losses = []\n","\n","log_interval = 100\n","n_epoch = 40\n","\n","\n","#The transform needs to live on the same device as the model and the data.\n","transform = transform.to(device)\n","\n","for epoch in range(1, n_epoch + 1):\n","    train(epoch, log_interval)\n","    val_loss, val_acc = evaluate(val_loader)\n","\n","    print(f'| Validation | val loss {val_loss:5.2f} | val accuracy {val_acc:.0f}%')\n","\n","    if val_loss < best_val_loss:\n","      with open(\"GRU-2_8kHz.pth\", 'wb') as f:\n","        torch.save(model, f)\n","      best_val_loss = val_loss\n","    scheduler.step()\n","\n","with open(\"GRU-2_8kHz.pth\", 'rb') as f:\n","  model = torch.load(f)\n","\n","with torch.no_grad():\n","  test_loss, test_acc = evaluate(test_loader)\n","\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | test accuracy {test_acc:.0f}%')\n","print('=' * 89)\n","\n","# Let's plot the training loss versus the number of iteration.\n","plt.plot(losses)\n","plt.title(\"training loss\")"]}],"metadata":{"accelerator":"GPU","colab":{"name":"GRU-2_8kHz.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"acb38c2ccc224967bf0e44159415414a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a422656a40854eb5b3225dd7cf2d1020","IPY_MODEL_6efaa17e51604c90822fcee20182982e","IPY_MODEL_1417227d21d44b31b461306b8c3450a7"],"layout":"IPY_MODEL_ba0352d0e7a84838ba101edd29469b86"}},"a422656a40854eb5b3225dd7cf2d1020":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08f334c186924258bca5018cc21522bd","placeholder":"​","style":"IPY_MODEL_db28b2fd3bf44fd297bcff8e3f9a3f40","value":"100%"}},"6efaa17e51604c90822fcee20182982e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd00c3038c984f6082b862976c60ad37","max":2428923189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc288b5de9c44f9f8d8cb8a66c3bf5cc","value":2428923189}},"1417227d21d44b31b461306b8c3450a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_108998e2831b44588a2e751f14f83382","placeholder":"​","style":"IPY_MODEL_1db21f9d62c64b14a6cc0738d7583a59","value":" 2.26G/2.26G [00:15&lt;00:00, 217MB/s]"}},"ba0352d0e7a84838ba101edd29469b86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08f334c186924258bca5018cc21522bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db28b2fd3bf44fd297bcff8e3f9a3f40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd00c3038c984f6082b862976c60ad37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc288b5de9c44f9f8d8cb8a66c3bf5cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"108998e2831b44588a2e751f14f83382":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db21f9d62c64b14a6cc0738d7583a59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}